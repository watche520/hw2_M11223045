{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights'):\n",
    "    os.makedirs('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(img, mask, size=256):\n",
    "    '''\n",
    "    Crop the image to square and resize to 256x256\n",
    "    '''\n",
    "    # crop to square\n",
    "    h, w = img.shape\n",
    "    # print(f'h: {h}, w: {w}')\n",
    "    if h > w:\n",
    "        img = img[:w, :]\n",
    "        mask = mask[:w, :]\n",
    "    else:\n",
    "        w_center = w // 2\n",
    "        img = img[:, w_center - h // 2: (w_center - h // 2)+h]\n",
    "        mask = mask[:, w_center - h // 2: (w_center - h // 2)+h]\n",
    "\n",
    "    # resize to 256x256\n",
    "    img = cv2.resize(img, (size, size))\n",
    "    mask = cv2.resize(mask, (size, size))\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "def data_augmentation(img, mask):\n",
    "    # CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    img_c = clahe.apply(img)\n",
    "    # horizontal flip\n",
    "    # https://claire-chang.com/2023/06/09/%E4%BD%BF%E7%94%A8opencv%E5%B0%87%E5%9C%96%E5%BD%A2%E8%BD%89%E6%AD%A3/\n",
    "    img_f = cv2.flip(img_c, flipCode=1)\n",
    "    mask_f = cv2.flip(mask, flipCode=1)\n",
    "    return img_c, img_f, mask_f\n",
    "\n",
    "def img_normalize(img, mask):\n",
    "    # 0~255 -> 0~1\n",
    "    img = img / 255\n",
    "\n",
    "    # 0~255 -> 0 or 1\n",
    "    # binary_mask = mask.copy()\n",
    "    # binary_mask[mask > 128] = 1\n",
    "    # binary_mask[mask <= 128] = 0\n",
    "    binary_mask = to_binary_mask(mask)\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    binary_mask = binary_mask.astype(np.float32)\n",
    "\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    binary_mask = np.expand_dims(binary_mask, axis=-1)\n",
    "\n",
    "    return img, binary_mask\n",
    "\n",
    "def to_binary_mask(mask):\n",
    "    max_value = np.max(mask)\n",
    "    binary_mask = mask.copy()\n",
    "    binary_mask[mask > max_value/2] = 1\n",
    "    binary_mask[mask <= max_value/2] = 0\n",
    "    return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_from_one_folder(folder_name, sub_folder_name):\n",
    "    imgs = []\n",
    "    masks = []\n",
    "\n",
    "    # img files list\n",
    "    file_list = os.listdir(os.path.join('ETT_v3', folder_name, sub_folder_name))\n",
    "\n",
    "    for file_num in range(len(file_list)):\n",
    "        print(f'loading {folder_name}/{sub_folder_name}: {file_num+1}/{len(file_list)}', end='\\r')\n",
    "\n",
    "        file_name = file_list[file_num]\n",
    "        img_path = os.path.join('ETT_v3', folder_name, sub_folder_name, file_name)\n",
    "        mask_path = os.path.join('ETT_v3', folder_name, sub_folder_name+'annot', file_name.replace('.jpg', '.png'))\n",
    "\n",
    "        if not (os.path.isfile(img_path)):\n",
    "            # img does not exist\n",
    "            print(f'file {img_path} not exist')\n",
    "            continue\n",
    "        elif not (os.path.isfile(mask_path)):\n",
    "            # same name mask does not exist\n",
    "            mask_path = mask_path.replace('img', 'mask')\n",
    "            if not (os.path.isfile(mask_path)):\n",
    "                # another name mask does not exist\n",
    "                print(f'file {mask_path} not exist')\n",
    "                continue\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # plt.subplot(3, 2, 1)\n",
    "        # plt.imshow(img, cmap='gray')\n",
    "        # plt.subplot(3, 2, 2)\n",
    "        # plt.imshow(mask, cmap='gray')\n",
    "\n",
    "        # crop and resize\n",
    "        img, mask = crop_and_resize(img, mask)\n",
    "\n",
    "        # data augmentation (CLAHE, horizontal flip)\n",
    "        img, img_f, mask_f = data_augmentation(img, mask)\n",
    "\n",
    "        # plt.subplot(3, 2, 3)\n",
    "        # plt.imshow(img, cmap='gray')\n",
    "        # plt.subplot(3, 2, 4)\n",
    "        # plt.imshow(mask, cmap='gray')\n",
    "        # plt.subplot(3, 2, 5)\n",
    "        # plt.imshow(img_f, cmap='gray')\n",
    "        # plt.subplot(3, 2, 6)\n",
    "        # plt.imshow(mask_f, cmap='gray')\n",
    "\n",
    "        # normalize\n",
    "        img, mask = img_normalize(img, mask)\n",
    "        img_f, mask_f = img_normalize(img_f, mask_f)\n",
    "\n",
    "        imgs.append(img)\n",
    "        masks.append(mask)\n",
    "        imgs.append(img_f)\n",
    "        masks.append(mask_f)\n",
    "\n",
    "    print('')\n",
    "\n",
    "    imgs = np.array(imgs)\n",
    "    masks = np.array(masks)\n",
    "\n",
    "    return imgs, masks\n",
    "\n",
    "# img = cv2.imread(r'ETT_v3\\Fold1\\train\\img_1.2.826.0.1.3680043.8.498.10017127632518130787233241824845923514.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# mask = cv2.imread(r'ETT_v3\\Fold1\\trainannot\\mask_1.2.826.0.1.3680043.8.498.10017127632518130787233241824845923514.png', cv2.IMREAD_GRAYSCALE)\n",
    "# plt.subplot(3, 2, 1)\n",
    "# plt.imshow(img, cmap='gray')\n",
    "# plt.subplot(3, 2, 2)\n",
    "# plt.imshow(mask, cmap='gray')\n",
    "# img, mask = crop_and_resize(img, mask)\n",
    "# img, img_f, mask_f = data_augmentation(img, mask)\n",
    "# plt.subplot(3, 2, 3)\n",
    "# plt.imshow(img, cmap='gray')\n",
    "# plt.subplot(3, 2, 4)\n",
    "# plt.imshow(mask, cmap='gray')\n",
    "# plt.subplot(3, 2, 5)\n",
    "# plt.imshow(img_f, cmap='gray')\n",
    "# plt.subplot(3, 2, 6)\n",
    "# plt.imshow(mask_f, cmap='gray')\n",
    "# img, mask = img_normalize(img, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegNet\n",
    "\n",
    "參考資料:[[TF-Keras]SegNet-Starter-Submission - Kaggle](https://www.kaggle.com/code/salmankhaliq22/tf-keras-segnet-starter-submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/salmankhaliq22/tf-keras-segnet-starter-submission\n",
    "\n",
    "def segnet_encoder_block(inputs, filters_num, kernel_size=3, padding='same', pool_size=2):\n",
    "    # first layer\n",
    "    x = keras.layers.Conv2D(filters_num, (kernel_size, kernel_size), padding=padding)(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    # second layer\n",
    "    x = keras.layers.Conv2D(filters_num, (kernel_size, kernel_size), padding=padding)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    # pooling layer\n",
    "    pool = keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size))(x)\n",
    "    return pool\n",
    "\n",
    "def segnet_decoder_block(inputs, filters_num, kernel_size=3, padding='same'):\n",
    "    # first layer\n",
    "    x = keras.layers.UpSampling2D(size=(2, 2))(inputs)\n",
    "    # second layer\n",
    "    x = keras.layers.Conv2D(filters_num, (kernel_size, kernel_size), padding=padding)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    # third layer\n",
    "    x = keras.layers.Conv2D(filters_num, (kernel_size, kernel_size), padding=padding)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def segnet(input_shape, classes_num=1):\n",
    "    input = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # encoder\n",
    "    pool1 = segnet_encoder_block(input, 64)\n",
    "    pool2 = segnet_encoder_block(pool1, 128)\n",
    "    pool3 = segnet_encoder_block(pool2, 256)\n",
    "    pool4 = segnet_encoder_block(pool3, 512)\n",
    "\n",
    "    # decoder\n",
    "    x = segnet_decoder_block(pool4, 512)\n",
    "    x = segnet_decoder_block(x, 256)\n",
    "    x = segnet_decoder_block(x, 128)\n",
    "    x = segnet_decoder_block(x, 64)\n",
    "\n",
    "    # output\n",
    "    output = keras.layers.Conv2D(classes_num, (1, 1), padding='same', activation='sigmoid')(x)\n",
    "\n",
    "    model = keras.models.Model(input, output)\n",
    "    return model\n",
    "\n",
    "def iou_ccalculate(y_true, y_pred, pixels_per_cm=72):\n",
    "    absolute_difference = tf.abs(y_true - y_pred)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    iou = intersection / union\n",
    "\n",
    "    mean_error = tf.reduce_mean(absolute_difference) / pixels_per_cm\n",
    "    accuracy_within_50mm = tf.reduce_mean(tf.cast(absolute_difference <= (pixels_per_cm / 2), tf.float32)) * 100\n",
    "    accuracy_within_1cm = tf.reduce_mean(tf.cast(absolute_difference <= pixels_per_cm, tf.float32)) * 100\n",
    "\n",
    "    return iou,mean_error, accuracy_within_50mm, accuracy_within_1cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_list = os.listdir('ETT_v3')\n",
    "# folders_list = folders_list[:1]\n",
    "\n",
    "for folder_name in folders_list:\n",
    "    # Folder 1 ~ Folder 6\n",
    "    # Load data\n",
    "    train_imgs, train_masks = load_img_from_one_folder(folder_name, 'train')\n",
    "    valid_imgs, valid_masks = load_img_from_one_folder(folder_name, 'val')\n",
    "    # test_imgs, test_masks = load_img_from_one_folder(folder_name, 'test')\n",
    "    # create model\n",
    "    model = segnet((256, 256, 1))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy')\n",
    "    # train model\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(os.path.join('weights', f'segnet_{folder_name}.h5'), save_best_only=True, save_weights_only=True, mode='min'),\n",
    "        keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ]\n",
    "    history = model.fit(train_imgs,\n",
    "                        train_masks,\n",
    "                        validation_data=(valid_imgs, valid_masks),\n",
    "                        epochs=50,\n",
    "                        batch_size=16)\n",
    "    if not os.path.exists(os.path.join('weights', f'segnet_{folder_name}.h5')):\n",
    "        model.save_weights(os.path.join('weights', f'segnet_{folder_name}.h5'))\n",
    "    else:\n",
    "        model.load_weights(os.path.join('weights', f'segnet_{folder_name}.h5'))\n",
    "\n",
    "    shutil.copyfile(os.path.join('weights', f'segnet_{folder_name}.h5'), os.path.join('/content/drive/MyDrive', f'segnet_{folder_name}.h5'))\n",
    "\n",
    "    print(f'Folder {folder_name} training done')\n",
    "\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Folder', 'IoU', 'Mean Error', 'Accuracy within 50mm', 'Accuracy within 1cm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_list = os.listdir('ETT_v3')\n",
    "\n",
    "for folder_name in folders_list:\n",
    "    # Folder 1 ~ Folder 6\n",
    "    # Load data\n",
    "    # train_imgs, train_masks = load_img_from_one_folder(folder_name, 'train')\n",
    "    # valid_imgs, valid_masks = load_img_from_one_folder(folder_name, 'val')\n",
    "    test_imgs, test_masks = load_img_from_one_folder(folder_name, 'test')\n",
    "    # load model\n",
    "    model = segnet((256, 256, 1))\n",
    "    model.load_weights(os.path.join('weights', f'segnet_{folder_name}.h5'))\n",
    "    #test model\n",
    "    pre_mask = model.predict(test_imgs, batch_size=2, verbose=0)\n",
    "    # calculate iou\n",
    "    iou_list = []\n",
    "    mean_error = []\n",
    "    accuracy_within_50mm = []\n",
    "    accuracy_within_1cm = []\n",
    "    for i in range(len(test_imgs)):\n",
    "        # test model\n",
    "        test_pred_masks = to_binary_mask(pre_mask[i])\n",
    "        iou, me, aw50, aw1 = iou_ccalculate(test_masks[i], test_pred_masks)\n",
    "        iou_list.append(iou)\n",
    "        mean_error.append(me)\n",
    "        accuracy_within_50mm.append(aw50)\n",
    "        accuracy_within_1cm.append(aw1)\n",
    "    iou = np.mean(iou_list)\n",
    "    mean_error = np.mean(mean_error)\n",
    "    accuracy_within_50mm = np.mean(accuracy_within_50mm)\n",
    "    accuracy_within_1cm = np.mean(accuracy_within_1cm)\n",
    "    print(f'Folder {folder_name} IoU: {iou:.10f}, mean_error: {mean_error:.10f}, accuracy_within_50mm: {accuracy_within_50mm}%, accuracy_within_1cm: {accuracy_within_1cm}%')\n",
    "    with open('result.csv', 'a+', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([folder_name, f'{iou:.10f}', f'{mean_error:.10f}', accuracy_within_50mm, accuracy_within_1cm])\n",
    "    tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
